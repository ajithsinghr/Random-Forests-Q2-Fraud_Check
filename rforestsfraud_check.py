# -*- coding: utf-8 -*-
"""RForestsFraud_check.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mzNoSln69mlhwgNnTkWtUMO9m3EPKuLZ
"""

from google.colab import files
uploaded = files.upload()

# importing libraries
import pandas as pd
from sklearn.tree import  DecisionTreeClassifier
from sklearn import tree
import matplotlib.pyplot as plt

df = pd.read_csv("Fraud_check.csv")
df

df.dtypes
df.isnull().sum()
df.shape
df.info()

# Data visualization
import seaborn as sns
import matplotlib.pyplot as plt


# Check Correlation amoung parameters
corr = df.corr()
fig, ax = plt.subplots(figsize=(8,8))

#  heatmap
sns.heatmap(corr, cmap = 'magma', annot = True, fmt = ".2f")
plt.xticks(range(len(corr.columns)), corr.columns)

plt.yticks(range(len(corr.columns)), corr.columns)

plt.show()

sns.countplot(df['Undergrad'])
plt.show()

sns.countplot(df['Marital.Status'])
plt.show()

sns.countplot(df['Urban'])

sns.pairplot(df,hue="Taxable.Income")
plt.show()

sns.pairplot(df)
plt.show()

# pip install category_encoders
#Encoding
import category_encoders as ce
encoder=ce.OrdinalEncoder(cols=['Undergrad', 'Marital.Status', 'Urban'])
df=encoder.fit_transform(df)
df.head()

#treating those who have taxable_income <= 30000 as "Risky" and others are "Good"
tax = []
for value in df["Taxable.Income"]:
    if value<=30000:
        tax.append("Risky")
    else:
        tax.append("Good")

df["tax"] = tax
df.head()

#splitting the data into x and y
x=df.drop(['Taxable.Income','tax'],axis=1)
y = df["tax"]
#Data Partition
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.30,random_state=(42))

#Random Forest
from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier(max_depth=6)
rfc.fit(x_train,y_train)

y_pred_train = rfc.predict(x_train)
y_pred_test = rfc.predict(x_test)

from sklearn.metrics import accuracy_score
ac1 = accuracy_score(y_train,y_pred_train)
print("training score:",ac1.round(2))
ac2 = accuracy_score(y_test,y_pred_test)
print("test score:",ac2.round(2))

#Decisiontree
from sklearn.tree import  DecisionTreeClassifier
model = DecisionTreeClassifier(criterion = 'entropy',max_depth=7)
model.fit(x_train,y_train)

y_pred_train = model.predict(x_train)
y_pred_test = model.predict(x_test)

from sklearn.metrics import accuracy_score
ac1 = accuracy_score(y_train,y_pred_train)
print("training score:",ac1.round(2))
ac2 = accuracy_score(y_test,y_pred_test)
print("test score:",ac2.round(2))

#boosting
from sklearn.ensemble import AdaBoostClassifier

ABC = AdaBoostClassifier(n_estimators=10,base_estimator=model
                        ,random_state=(25))

ABC.fit(x_train, y_train)
Y_pred_train = ABC.predict(x_train) 
Y_pred_test = ABC.predict(x_test)

from sklearn.metrics import accuracy_score
ac1 = accuracy_score(y_train,Y_pred_train)
print("training score:",ac1.round(2))
ac2 = accuracy_score(y_test,Y_pred_test)
print("test score:",ac2.round(2))

#bagging
from sklearn.ensemble import BaggingClassifier
bag = BaggingClassifier(base_estimator=model ,
                       n_estimators=150,
                       max_samples=0.6,random_state=10, max_features=0.7)

bag.fit(x_train, y_train)
Y_pred_train = bag.predict(x_train) 
Y_pred_test = bag.predict(x_test)

from sklearn.metrics import accuracy_score
ac1 = accuracy_score(y_train,Y_pred_train)
print("training score:",ac1.round(2))
ac2 = accuracy_score(y_test,Y_pred_test)
print("test score:",ac2.round(2))

